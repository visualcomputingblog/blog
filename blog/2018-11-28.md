## 【论文阅读】一种新型简化网络方法 —— 利用线性冗余训练小网络（ExpandNets: Exploiting Linear Redundancy to Train Small Networks）

### Contribution
这是一篇挺有意思的论文，论文提出了一种新的训练方法来提高小网络的精度。知识迁移用大网络指导小网络训练是现在比较流行的提高小网络精度的方法，论文从参数冗余有助于学习的角度考虑给出另外一种解决方案，具体做法就是将一个线性层扩展成多个线性层得到一个扩展后的网络，训练好之后的扩展网络可以压缩回小网络，实验表明扩展网络的精度明显优于直接训练小网络。进一步的，论文运用知识迁移方法可以进一步提高网络性能。总体而言，论文创新点在于如下几个方面：

- 简单的策略对小网络引入冗余，提高网络学习性能，并且在数学上引入的冗余可以压缩回去。
- 论文提出了一种有效的网络初始化方法初始化扩展网络。

### Methods
论文首先做了个小实验证明论文的假设，即线性冗余是有益的。如下图所示:![toy](E:\github\blog\image\2018-11-28\loss-toy.jpg)
当使用两个参数去拟合时，有51.5%的概率落在好的局部极值上，而一个参数却从未落在最优的局部极值上。

基于上述假设，论文给出了对小网络增加线性冗余的方法。具体的包含三种情况：

- FC层扩展，一般的FC层可以用表示为![fc](E:\github\blog\image\2018-11-28\fc.jpg),一个FC层扩展为多个FC层，就是对参数W进行扩展，即![fc-w](E:\github\blog\image\2018-11-28\fc-w.jpg)。
- 卷积层扩展，对于卷积层的扩展不能直接叠加多个KxK的卷积层，除非K=1，因为叠加多个KxK会改变网络感受野大小。所以对卷积层的扩展是叠加两个1x1加一个KxK，
- 卷积核扩展，对于大的卷积核（>3)可以叠加多个小核，保留感受野大小的同时增加网络表达能力，例如一个5x5的卷积核可以用两个3x3的卷积核得到等价的感受野。

具体的小网络扩展结构为：![Networks](E:\github\blog\image\2018-11-28\network.jpg)

对于网络训练来说，初始化方法对网络收敛结果有很大影响，论文针对ExpandNet设计了一种初始化方法，即首先训练一个非线性版本的ExpandNet（在每个增加的线性层后面增加非线性激活函数），然后用训好的非线性ExpandNet初始化网络。

### Experiments and Conclusion
论文在CIFAR-10和CIFAR-100上进行了实验，卷积层扩展+FC层扩展+初始化方法在CIFAR-10上带来了明显提升。![experiment1](E:\github\blog\image\2018-11-28\experiment1.jpg)

知识迁移方法能带来进一步的精度提升，论文实验采用ResNet18作为teacher网络的实验结果为：![experiment2](E:\github\blog\image\2018-11-28\experiment2.jpg)
也可以用ExpandNet的非线性版本作为teacher，其结果也能比直接指导小网络效果要好。
![experiment3](E:\github\blog\image\2018-11-28\experiment3.jpg)

### 其它
论文没有明确给出将ExpandNet压缩回小网络的公式，FC层合并简单，对卷积层来说，卷积计算满足结合律，叠加多个卷积最后压缩的时候对卷积核做卷积计算就行。对于卷积核扩展情况，多个小核的计算量貌似比单个大核的计算量，所以对于大核的情况，直接用扩展部分是否更有优势？
