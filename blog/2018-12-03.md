## 【论文分享】图像分类训练技巧（Bag of Tricks for Image Classification with Convolutional Neural Networks）

### Contribution
今天介绍的论文是Amazon出品的图像分类训练技巧，很多图像分类研究都着重放在训练过程的改进上，比如数据预处理和优化方法，在这些论文里，很多改进对于实现细节都轻描淡写或者简单的开源代码。在这篇论文里，作者对这些改进进行了整理，并通过实验评估它们对最后精度的影响。实验表明，整合这些改进，可以明显改善cnn模型的效果，比如ResNet50在ImageNet验证集上的top-1准确度可以从75.3%提高至79.29%。同时，论文还通过实验表明这些在图像分类上的改进有助于迁移至其它任务比如物体检测和语义分割。

### Method
Baseline， 训练和测试阶段的预处理方法一般不一样。在训练阶段遵循如下步骤：
> 1. 随即选择一张图片，并解码成32-bit浮点数。
> 2. 随机crop出一个矩形区域，长宽比在[3/4, 4/3]之间，面积在整图的[8%,100%]之间，然后resize到224*224
> 3. 以0.5的概率随机水平翻转
> 4. hue, saturation, brightnes 在[0.6, 1.4]之间随机变换
> 5. 增加PCA噪声，噪声服从高斯分布N(0, 0.1)
> 6. 图片通道减（123.68， 116.779， 103.939），除以(58.393, 57.12, 57.375)

在测试阶段，图片短边resize到256以保持长宽比，然后corp出224*224的图片，图片归一化同步骤6.

conv和fc层的权重都用Xavier初始化方法，优化方法采用NAG（Nesterov Accerlerated Gradient），模型采用8块V100迭代120个epochs，batchsize为256，lr初始为0.1，然后在30th，60th，90th处下降10倍。

### Experiments and Conclusion
- 论文对比了三个网络ResNet-50，Inception-V3和MobileNet。对于Inception-V3,输入图片resize到299*299。训练集采用ISLVRC2012。作为baseline， ResNet-50比参考方法略好，Inception-V3和MobileNet略低。
- 大batch训练，大batch会降低收敛速度。
- 线性增大学习率，大batch不改变期望的梯度放心，但是会减小方向变化的方差，换句话说是减小梯度的噪声，所以可以采用更大的学习率，Goyal提出采用线性增长的学习率。
- 学习率热启动，在训练开始阶段，参数都是随机初始化的，距离最终的解还比较远，使用太大的学习率会导致的数值不稳定，在热启动阶段，采用比较小的学习率作为开始然后切换至初始学习率。Goyal提出采用从0到初始学习率随着迭代次数线性增加的学习率。
- zero gamma, resnet最后一个bn层的gamma用0作为初始化，可以在初始阶段更容易收敛。
- bias不加正则，conv和fc的bias不加正则，bn的gamma和bias也不加正则。
- 低精度训练，FP32改用FP16训练，速度提高2~3倍，但是同时因为精度问题也可能对最后的准确率有影响。Micikevicius提出在训练阶段保留一个FP32的参数副本做参数更新，或者在loss层增加一个尺度做个align。
- 实验结果1
> Resnet-50 batch256+FP32 vs batch1024+FP16， 训练速度13.3 min/epoch vs 4.4min/epoch，加上如上训练策略，最终结果比baseline高0.5%，单纯的batchsize从256提高至1024，top-1准确度下降0.9%，加上其它策略填平了这个差距，FP32和FP16之间无精度差异，具体结果如图所示
![exp1](E:\github\blog\blog\image\2018-12-03\table1.jpg)

-模型调整，具体调整如下图
![network](E:\github\blog\blog\image\2018-12-03\network.jpg)
实验结果2如下图
![exp2](E:\github\blog\blog\image\2018-12-03\table2.jpg)

- 学习率按cosine曲线下降
![cosine](E:\github\blog\blog\image\2018-12-03\cosine.jpg)

- 标签平滑
![label](E:\github\blog\blog\image\2018-12-03\label.jpg)

- 知识蒸馏，大模型指导小模型，这是在分类领域里使用很多的方法了。

- 混合训练，用两个样本合成一个新样本。
实验结果3如下图
![exp3](E:\github\blog\blog\image\2018-12-03\table3.jpg)

- 迁移学习，使用更高精度的分类模型预训练物体检测和语义分割，可以获得更高的精度。

### 其它
关于最后迁移学习的结果，这与Kaiming He近期的论文“Rethinking ImageNet Pre-training”似乎有些冲突。
